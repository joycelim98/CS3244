{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import the relevant libraries here\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here we write a function to import the data\n",
    "\"\"\"\n",
    "\n",
    "def get_dataset(size = \"large\"):\n",
    "    if size == \"large\":\n",
    "        url = 'https://raw.githubusercontent.com/SarahTaaherBonna/CS3244Project/master/ExtractedFeaturesDataset/extracted_features_LARGE_v4.csv?token=AIL356YJT3QGXUUCD6GD2Q253IX2K' \n",
    "        df = pd.read_csv(url)\n",
    "        return df\n",
    "    else:\n",
    "        url = \"https://raw.githubusercontent.com/SarahTaaherBonna/CS3244Project/master/ExtractedFeaturesDataset/extracted_features_SMALL_v6.csv?token=AIL356ZI42NVMF2AOEVW5GS53IYPG\"\n",
    "        df = pd.read_csv(url)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = get_dataset()\n",
    "# test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst1 = [\"proportion_exclamations_title\",'proportion_exclamations_content', 'proportion_question_title', 'proportion_question_content', 'proportion_allcaps_title'\n",
    "        ,'proportion_allcaps_content', 'proportion_contractions_title', 'proportion_contractions_content', 'proportion_words_in_title_in_content', 'sentiment_title'\n",
    "        ,\"starts_with_number_title\", \"len_longest_word_content\", \"starts_with_5W1H_content\", \"starts_with_5W1H_title\"\n",
    "        ,\"proportion_number_proper_nouns_in_content\", \"proportion_of_stop_words_in_title\"]\n",
    "\n",
    "# sampling_method = \"oversample\"\n",
    "sampling_method = \"undersample\"\n",
    "# sampling_method = None\n",
    "\n",
    "def get_train_and_test(df_size = \"large\", sampling_method = None, percentage = 0.2):\n",
    "    def generate_balanced_dataset_undersampling(df): #nested helper function to generate balanced datasets\n",
    "        only_nonclickbait = df.loc[df['label'] == 0]\n",
    "        only_clickbait = df.loc[df['label'] == 1]\n",
    "\n",
    "        while len(only_nonclickbait)!= len(only_clickbait):\n",
    "            to_drop = random.randint(0, len(only_nonclickbait) - 1)\n",
    "            only_nonclickbait = only_nonclickbait.drop(only_nonclickbait.index[to_drop])\n",
    "\n",
    "        frames = [only_clickbait, only_nonclickbait]\n",
    "        final_balanced_dataset = pd.concat(frames)\n",
    "        print(\"Done generating balanced dataset via undersampling\")\n",
    "        return final_balanced_dataset\n",
    "\n",
    "    df = get_dataset(size = df_size) \n",
    "    \n",
    "    if sampling_method == \"oversample\":\n",
    "        X = df[lst1]\n",
    "        y = df['label'].astype(int)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = percentage, random_state = 51)\n",
    "#         X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = percentage, random_state = 51)\n",
    "        ros = RandomOverSampler(random_state=0)\n",
    "#         X_train_resized = X_train.reshape(-1,1)\n",
    "        X_train, y_train = ros.fit_resample(X_train, y_train)\n",
    "        X_train = pd.DataFrame(X_train)\n",
    "        X_train.columns = lst1\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    elif sampling_method == \"undersample\":\n",
    "        df_balanced = generate_balanced_dataset_undersampling(df)\n",
    "        df_balanced = df_balanced.sample(frac=1)\n",
    "        X = df_balanced[lst1]\n",
    "        y = df_balanced['label'].astype(int)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = percentage, random_state=51)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    else:\n",
    "        print(\"Neither undersampling nor oversampling specified\")\n",
    "        X = df[lst1]\n",
    "        y = df['label'].astype(int)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = percentage, random_state=51)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "if sampling_method == \"oversample\":\n",
    "    X_train, X_test, y_train, y_test = get_train_and_test(df_size = \"large\", sampling_method = sampling_method)\n",
    "    pass\n",
    "elif sampling_method == \"undersample\":\n",
    "    X_train, X_test, y_train, y_test = get_train_and_test(df_size = \"large\", sampling_method = \"undersample\")\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = get_train_and_test(df_size = \"large\", sampling_method = \"None\")\n",
    "    pass\n",
    "\n",
    "print(\"No. of training examples:\", len(X_train))\n",
    "print(\"No. of testing examples:\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Since Naive Bayes assumes that all the input features are conditionally independent, we find the correlation between the input features\n",
    "\"\"\"\n",
    "def plot_corr_heatmap(fig_name = None):\n",
    "    plt.subplots(figsize=(10,10))\n",
    "    ax = plt.axes()\n",
    "    ax.set_title(\"Article Characteristic Correlation Heatmap\")\n",
    "    # Generating a correlation heatmap\n",
    "    corr = X_train.corr()\n",
    "    sns_plot = sn.heatmap(corr, \n",
    "                xticklabels=corr.columns.values,\n",
    "                yticklabels=corr.columns.values,\n",
    "                annot=True, \n",
    "               cmap=\"coolwarm\")\n",
    "    fig = sns_plot.get_figure()\n",
    "    plt.show()\n",
    "\n",
    "    if fig_name:\n",
    "        fig.savefig(fig_name)\n",
    "        print(\"figure saved\")\n",
    "\n",
    "if sampling_method == \"undersample\":\n",
    "    plot_corr_heatmap(\"Article Characteristic Correlation Heatmap (undersampling).png\")\n",
    "elif sampling_method == \"oversample\":\n",
    "    plot_corr_heatmap(\"Article Characteristic Correlation Heatmap (oversampling).png\")\n",
    "else:\n",
    "    plot_corr_heatmap(\"Article Characteristic Correlation Heatmap imbalanced dataset.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Finding if the features follow some sort of bell-curve shape, to justify the use of Gaussian Naive Bayes\n",
    "\"\"\"\n",
    "def plot_figure_distribution(fname = None):\n",
    "    X_train.hist(column = lst1, bins = 75, figsize=(20,20), color='#86bf91', zorder=2, rwidth=0.9);\n",
    "    \n",
    "    if fname:\n",
    "        plt.savefig(fname)\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "if sampling_method == \"undersample\":\n",
    "    plot_figure_distribution(\"Individual feature distribution (undersampling)\")\n",
    "elif sampling_method == \"oversample\":\n",
    "    plot_figure_distribution(\"Individual feature distribution (oversampling)\" )\n",
    "else:\n",
    "    plot_figure_distribution(\"Individual feature distribution imbalanced dataset.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluating the model\n",
    "\"\"\"\n",
    "\n",
    "model = GaussianNB()\n",
    "y_train_pred = model.fit(X_train, y_train).predict(X_train)\n",
    "y_test_pred = model.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Training performance\")\n",
    "print(\"Number of mislabelled points out of a total %d points : %d\"\n",
    "       % (y_train.shape[0],(y_train != y_train_pred).sum()))\n",
    "print(\"\\n\")\n",
    "print(\"Test performance\")\n",
    "print(\"Number of mislabelled points out of a total %d points : %d\"\n",
    "       % (y_test.shape[0],(y_test != y_test_pred).sum()))\n",
    "\n",
    "cm_test = confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cm_n_calculate_F1_score(confusion_matrix, normalise = True, fig_name = None, method_name = \"\"):\n",
    "\n",
    "    TN = confusion_matrix[0][0]\n",
    "    TP = confusion_matrix[1][1]\n",
    "    FP = confusion_matrix[0][1]\n",
    "    FN = confusion_matrix[1][0]\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    F1_score = 2 * precision * recall / (precision + recall)\n",
    "    print(\"The F1 score is:\", F1_score)\n",
    "    \n",
    "    cm = [[0,0],[0,0]]\n",
    "    if normalise:\n",
    "        cm[0][0] = TN / (TN + FP)\n",
    "        cm[0][1] = FP / (TN + FP)\n",
    "        cm[1][0] = FN / (FN + TP)\n",
    "        cm[1][1] = TP / (FN + TP)\n",
    "        plt.title('Normalised Confusion matrix ' + method_name, fontsize = 14)\n",
    "    else:\n",
    "        cm[0][0] = TN\n",
    "        cm[0][1] = FP\n",
    "        cm[1][0] = FN\n",
    "        cm[1][1] = TP\n",
    "        plt.title('Confusion matrix ' + method_name , fontsize = 14)\n",
    "        \n",
    "    labels = ['Non-Clickbait', 'Clickbait']\n",
    "    df_cm = pd.DataFrame(cm, index = labels, columns = labels,)\n",
    "    sns_plot = sn.heatmap(df_cm, annot=True, cmap=plt.cm.Blues, annot_kws={\"size\": 14})\n",
    "    fig = sns_plot.get_figure()\n",
    "    plt.xlabel('Predicted value', fontsize = 16)\n",
    "    plt.ylabel('True value', fontsize = 14)\n",
    "    plt.show()\n",
    "    \n",
    "    if fig_name:\n",
    "        fig.savefig(fig_name)\n",
    "        print(\"figure saved\")\n",
    "        \n",
    "if sampling_method == \"undersample\":\n",
    "    plot_cm_n_calculate_F1_score(cm_test, fig_name = \"Naive bayes normalised confusion matrix (undersampling).png\", method_name = \"Naive Bayes\")\n",
    "elif sampling_method == \"oversample\":\n",
    "    plot_cm_n_calculate_F1_score(cm_test, fig_name = \"Naive bayes normalised confusion matrix (uosampling).png\", method_name = \"Naive Bayes\")\n",
    "else:\n",
    "    plot_cm_n_calculate_F1_score(cm_test, fig_name = \"Naive bayes normalised confusion matrix (undersampling).png\", method_name = \"Naive Bayes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
