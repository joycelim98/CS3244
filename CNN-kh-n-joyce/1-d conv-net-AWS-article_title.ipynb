{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from load_dataset_title import get_dataset\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import FastText as ft\n",
    "\n",
    "from keras import layers, models, optimizers\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D,MaxPooling1D,Dropout,GlobalMaxPool1D,SpatialDropout1D,AveragePooling1D,GlobalAveragePooling1D\n",
    "\n",
    "\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python 3.6.4\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `load_fasttext_format` (use load_facebook_vectors (to use pretrained embeddings) or load_facebook_model (to continue training with the loaded full model, more RAM) instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "c:\\python 3.6.4\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "embed_model2 = ft.load_fasttext_format(\"cc.en.300.bin\")\n",
    "\n",
    "vec_dim = len(embed_model2[\"cat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading large document\n",
      "19539\n",
      "hi\n"
     ]
    }
   ],
   "source": [
    "df = get_dataset(size = \"large\") #use small for small dataset, large for large dataset. Will print the number of articles loaded.\n",
    "\n",
    "X = df[\"title\"]\n",
    "y = df['label']\n",
    "\n",
    "percentage = 0.2\n",
    "\n",
    "# (random_state): we use a fixed random seed so we get the same results every time.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = percentage, random_state=51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237\n"
     ]
    }
   ],
   "source": [
    "##Finding length of longest article\n",
    "\n",
    "def get_max_length():\n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(X)\n",
    "    encoded_X = t.texts_to_sequences(X)\n",
    "    max_length = 0\n",
    "    for i in encoded_X:\n",
    "        max_length = max(len(i), max_length)\n",
    "    \n",
    "    return max_length + 1\n",
    "\n",
    "input_length = get_max_length()\n",
    "print(input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_input(array):\n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(array)\n",
    "    vocab_size = len(t.word_index) + 1\n",
    "    encoded_X = t.texts_to_sequences(array)\n",
    "\n",
    "    ##input_length is a global variable\n",
    "    padded_X = pad_sequences(encoded_X, maxlen = input_length,\n",
    "                         padding = \"post\")\n",
    "    \n",
    "    return (padded_X, t)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python 3.6.4\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Create the embedding matrix using the training data, as well as the padded X_train data\n",
    "padded_X_train, t_train = pre_process_input(X_train)\n",
    "vocab_size = len(t_train.word_index) + 1\n",
    "input_length = len(padded_X_train[0])\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, vec_dim))\n",
    "for word,i in t_train.word_index.items():\n",
    "    embedding_vector = embed_model2[word]\n",
    "    embedding_matrix[i] = embedding_vector\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the padded X_test data\n",
    "padded_X_test = pre_process_input(X_test)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First model: Just logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 237, 300)          7441500   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 71100)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 71101     \n",
      "=================================================================\n",
      "Total params: 7,512,601\n",
      "Trainable params: 71,101\n",
      "Non-trainable params: 7,441,500\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 12504 samples, validate on 3126 samples\n",
      "Epoch 1/50\n",
      "12504/12504 [==============================] - 3s 206us/step - loss: 0.5277 - accuracy: 0.7678 - val_loss: 0.4823 - val_accuracy: 0.7802\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.48226, saving model to C:\\Users\\Admin\\Desktop\\CS3244Project\\CNN-kh-n-joyce/model1_titleweights.01-0.482.hdf5\n",
      "Epoch 2/50\n",
      "12504/12504 [==============================] - 2s 187us/step - loss: 0.4771 - accuracy: 0.7846 - val_loss: 0.4740 - val_accuracy: 0.7901\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.48226 to 0.47403, saving model to C:\\Users\\Admin\\Desktop\\CS3244Project\\CNN-kh-n-joyce/model1_titleweights.02-0.474.hdf5\n",
      "Epoch 3/50\n",
      "12504/12504 [==============================] - 2s 186us/step - loss: 0.4587 - accuracy: 0.7959 - val_loss: 0.4694 - val_accuracy: 0.7911\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.47403 to 0.46941, saving model to C:\\Users\\Admin\\Desktop\\CS3244Project\\CNN-kh-n-joyce/model1_titleweights.03-0.469.hdf5\n",
      "Epoch 4/50\n",
      "12504/12504 [==============================] - 2s 189us/step - loss: 0.4449 - accuracy: 0.8033 - val_loss: 0.4688 - val_accuracy: 0.7905\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.46941 to 0.46878, saving model to C:\\Users\\Admin\\Desktop\\CS3244Project\\CNN-kh-n-joyce/model1_titleweights.04-0.469.hdf5\n",
      "Epoch 5/50\n",
      "12504/12504 [==============================] - 2s 186us/step - loss: 0.4337 - accuracy: 0.8089 - val_loss: 0.4692 - val_accuracy: 0.7908\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.46878\n",
      "Epoch 6/50\n",
      "12504/12504 [==============================] - 2s 185us/step - loss: 0.4250 - accuracy: 0.8132 - val_loss: 0.4721 - val_accuracy: 0.7898\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.46878\n",
      "Epoch 7/50\n",
      "12504/12504 [==============================] - 2s 188us/step - loss: 0.4169 - accuracy: 0.8145 - val_loss: 0.4715 - val_accuracy: 0.7911\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.46878\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x25a46650c18>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hyper-Params\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "patience = 3\n",
    "\n",
    "#Define model\n",
    "model1 = Sequential()\n",
    "e = Embedding(vocab_size, vec_dim, weights = [embedding_matrix],\n",
    "              input_length = input_length, trainable = False)\n",
    "model1.add(e)\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "model1.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "### summarize the model\n",
    "print(model1.summary())\n",
    "\n",
    "### Create model checkpoint\n",
    "output_dir = cwd\n",
    "modelcheckpoint = ModelCheckpoint(filepath = output_dir + \"/model1_titleweights.{epoch:02d}-{val_loss:.3f}.hdf5\",\n",
    "                                 monitor='val_loss', verbose=1, save_best_only=True)\n",
    "callbacks_list = [modelcheckpoint, EarlyStopping(monitor = \"val_loss\", patience = patience)]\n",
    "\n",
    "### fit the model\n",
    "model1.fit(padded_X_train, y_train, batch_size = batch_size, epochs=epochs, verbose=1, validation_split = 0.2, callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'accuracy']\n",
      "3908/3908 [==============================] - 0s 118us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5595763912586497, 0.7558853626251221]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Evaluate the model\n",
    "print(model1.metrics_names)\n",
    "model1.evaluate(x=padded_X_test, y=y_test, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second model: Coming soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third model: 1-D convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 237, 300)          7441500   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_2 (Spatial (None, 237, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 237, 100)          90100     \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 237, 100)          30100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 118, 100)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 118, 150)          45150     \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 118, 150)          67650     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 59, 150)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 59, 150)           67650     \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 59, 150)           67650     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 29, 150)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 29, 150)           67650     \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 29, 150)           67650     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 151       \n",
      "=================================================================\n",
      "Total params: 7,945,251\n",
      "Trainable params: 503,751\n",
      "Non-trainable params: 7,441,500\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 12504 samples, validate on 3126 samples\n",
      "Epoch 1/20\n",
      "12504/12504 [==============================] - 41s 3ms/step - loss: 0.5310 - accuracy: 0.7636 - val_loss: 0.4632 - val_accuracy: 0.7882\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.46324, saving model to C:\\Users\\Admin\\Desktop\\CS3244Project\\CNN-kh-n-joyce/model3_titleweights.01-0.463.hdf5\n",
      "Epoch 2/20\n",
      "12504/12504 [==============================] - 41s 3ms/step - loss: 0.4957 - accuracy: 0.7815 - val_loss: 0.4609 - val_accuracy: 0.7930\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.46324 to 0.46092, saving model to C:\\Users\\Admin\\Desktop\\CS3244Project\\CNN-kh-n-joyce/model3_titleweights.02-0.461.hdf5\n",
      "Epoch 3/20\n",
      "12504/12504 [==============================] - 41s 3ms/step - loss: 0.4892 - accuracy: 0.7869 - val_loss: 0.4713 - val_accuracy: 0.7908\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.46092\n",
      "Epoch 4/20\n",
      "12504/12504 [==============================] - 41s 3ms/step - loss: 0.4722 - accuracy: 0.7911 - val_loss: 0.4595 - val_accuracy: 0.7959\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.46092 to 0.45948, saving model to C:\\Users\\Admin\\Desktop\\CS3244Project\\CNN-kh-n-joyce/model3_titleweights.04-0.459.hdf5\n",
      "Epoch 5/20\n",
      "12504/12504 [==============================] - 41s 3ms/step - loss: 0.4645 - accuracy: 0.7952 - val_loss: 0.4649 - val_accuracy: 0.7962\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.45948\n",
      "Epoch 6/20\n",
      "12504/12504 [==============================] - 41s 3ms/step - loss: 0.4569 - accuracy: 0.7961 - val_loss: 0.4593 - val_accuracy: 0.7972\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.45948 to 0.45932, saving model to C:\\Users\\Admin\\Desktop\\CS3244Project\\CNN-kh-n-joyce/model3_titleweights.06-0.459.hdf5\n",
      "Epoch 7/20\n",
      "12504/12504 [==============================] - 41s 3ms/step - loss: 0.4395 - accuracy: 0.8004 - val_loss: 0.4688 - val_accuracy: 0.7818\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.45932\n",
      "Epoch 8/20\n",
      "12504/12504 [==============================] - 41s 3ms/step - loss: 0.4338 - accuracy: 0.8037 - val_loss: 0.4833 - val_accuracy: 0.7885\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.45932\n",
      "Epoch 9/20\n",
      "12504/12504 [==============================] - 43s 3ms/step - loss: 0.4185 - accuracy: 0.8085 - val_loss: 0.4655 - val_accuracy: 0.7905\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.45932\n",
      "Epoch 10/20\n",
      "12504/12504 [==============================] - 43s 3ms/step - loss: 0.4051 - accuracy: 0.8153 - val_loss: 0.4852 - val_accuracy: 0.7793\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.45932\n",
      "Epoch 11/20\n",
      "12504/12504 [==============================] - 43s 3ms/step - loss: 0.3889 - accuracy: 0.8241 - val_loss: 0.4812 - val_accuracy: 0.7809\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.45932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x25a50923550>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hyper-Params\n",
    "epochs = 20\n",
    "batch_size = 32 #for stochastic gradient descent\n",
    "drop_embed = 0.5\n",
    "\n",
    "n_dense = 256\n",
    "dropout = 0.5\n",
    "\n",
    "n_conv_layer1 = 100\n",
    "n_conv_layer2 = 150\n",
    "n_conv_layer3 = 150\n",
    "n_conv_layer4 = 150\n",
    "#n_conv_layer5 = 100\n",
    "k_conv = 3\n",
    "\n",
    "#Define model\n",
    "\n",
    "model3 = Sequential()\n",
    "e = Embedding(vocab_size, vec_dim, weights = [embedding_matrix],\n",
    "              input_length = input_length, trainable = False)\n",
    "model3.add(e)\n",
    "model3.add(SpatialDropout1D(drop_embed))\n",
    "model3.add(Conv1D(filters = n_conv_layer1, kernel_size = k_conv, activation = \"relu\", padding = \"same\"))\n",
    "model3.add(Conv1D(filters = n_conv_layer1, kernel_size = k_conv, activation = \"relu\", padding = \"same\"))\n",
    "model3.add(MaxPooling1D(2))\n",
    "model3.add(Conv1D(filters = n_conv_layer2, kernel_size = k_conv, activation = \"relu\", padding = \"same\"))\n",
    "model3.add(Conv1D(filters = n_conv_layer2, kernel_size = k_conv, activation = \"relu\", padding = \"same\"))\n",
    "model3.add(MaxPooling1D(2))\n",
    "model3.add(Conv1D(filters = n_conv_layer3, kernel_size = k_conv, activation = \"relu\", padding = \"same\"))\n",
    "model3.add(Conv1D(filters = n_conv_layer3, kernel_size = k_conv, activation = \"relu\", padding = \"same\"))\n",
    "model3.add(MaxPooling1D(2))\n",
    "model3.add(Conv1D(filters = n_conv_layer4, kernel_size = k_conv, activation = \"relu\", padding = \"same\"))\n",
    "model3.add(Conv1D(filters = n_conv_layer4, kernel_size = k_conv, activation = \"relu\", padding = \"same\"))\n",
    "#model3.add(MaxPooling1D(2))\n",
    "#model3.add(Conv1D(filters = n_conv_layer5, kernel_size = k_conv, activation = \"relu\", padding = \"same\"))\n",
    "#model3.add(Conv1D(filters = n_conv_layer5, kernel_size = k_conv, activation = \"relu\", padding = \"same\"))\n",
    "model3.add(GlobalMaxPool1D())\n",
    "model3.add(Dropout(dropout))\n",
    "#model3.add(Dense(n_dense, activation = \"relu\"))\n",
    "#model3.add(Dropout(dropout))\n",
    "model3.add(Dense(1, activation = \"sigmoid\"))\n",
    "model3.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "#              loss = \"mean_squared_error\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "### summarize the model\n",
    "print(model3.summary())\n",
    "\n",
    "### Create model checkpoint\n",
    "output_dir = cwd\n",
    "modelcheckpoint = ModelCheckpoint(filepath = output_dir + \"/model3_titleweights.{epoch:02d}-{val_loss:.3f}.hdf5\",\n",
    "                                 monitor='val_loss', verbose=1, save_best_only=True)\n",
    "callbacks_list = [modelcheckpoint, EarlyStopping(monitor = \"val_loss\", patience = 5)]\n",
    "\n",
    "#callbacks_list_no_chkpt = [EarlyStopping(monitor = \"val_loss\", patience = 2)]\n",
    "\n",
    "### fit the model\n",
    "model3.fit(padded_X_train, y_train, batch_size = batch_size, epochs=epochs, verbose=1, validation_split = 0.2, callbacks = callbacks_list)\n",
    "\n",
    "#fit with no checkpointing\n",
    "#model3.fit(padded_X_train, y_train, batch_size = batch_size, epochs=epochs, verbose=1, validation_split = 0.1, callbacks = callbacks_list_no_chkpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'accuracy']\n",
      "3908/3908 [==============================] - 4s 941us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5529692149516138, 0.7589560151100159]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Evaluate the best model\n",
    "model3_loaded = load_model(\"model3_titleweights.06-0.459.hdf5\")\n",
    "print(model3_loaded.metrics_names)\n",
    "model3_loaded.evaluate(x=padded_X_test, y=y_test, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
