{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from load_dataset_post_text import get_dataset\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import FastText as ft\n",
    "\n",
    "from keras import layers, models, optimizers\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D,MaxPooling1D,Dropout,GlobalMaxPool1D,SpatialDropout1D,AveragePooling1D,GlobalAveragePooling1D\n",
    "\n",
    "\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python 3.6.4\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `load_fasttext_format` (use load_facebook_vectors (to use pretrained embeddings) or load_facebook_model (to continue training with the loaded full model, more RAM) instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "c:\\python 3.6.4\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "embed_model2 = ft.load_fasttext_format(\"cc.en.300.bin\")\n",
    "\n",
    "vec_dim = len(embed_model2[\"cat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading large document\n",
      "19485\n"
     ]
    }
   ],
   "source": [
    "df = get_dataset(size = \"large\") #use small for small dataset, large for large dataset. Will print the number of articles loaded.\n",
    "\n",
    "X = df[\"postText\"].astype(str)\n",
    "y = df['label']\n",
    "\n",
    "percentage = 0.2\n",
    "\n",
    "# (random_state): we use a fixed random seed so we get the same results every time.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = percentage, random_state=51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "##Finding length of longest article\n",
    "\n",
    "def get_max_length():\n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(X)\n",
    "    encoded_X = t.texts_to_sequences(X)\n",
    "    max_length = 0\n",
    "    for i in encoded_X:\n",
    "        max_length = max(len(i), max_length)\n",
    "    \n",
    "    return max_length + 1\n",
    "\n",
    "input_length = get_max_length()\n",
    "print(input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_input(array):\n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(array)\n",
    "    vocab_size = len(t.word_index) + 1\n",
    "    encoded_X = t.texts_to_sequences(array)\n",
    "\n",
    "    ##input_length is a global variable\n",
    "    padded_X = pad_sequences(encoded_X, maxlen = input_length,\n",
    "                         padding = \"post\")\n",
    "    \n",
    "    return (padded_X, t)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the padded X_train data and the padded X_test data\n",
    "padded_X_train, t_train = pre_process_input(X_train)\n",
    "vocab_size = len(t_train.word_index) + 1\n",
    "input_length = len(padded_X_train[0])\n",
    "\n",
    "padded_X_test = pre_process_input(X_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python 3.6.4\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "#Create the embedding matrix using the training data\n",
    "embedding_matrix = np.zeros((vocab_size, vec_dim))\n",
    "for word,i in t_train.word_index.items():\n",
    "    embedding_vector = embed_model2[word]\n",
    "    embedding_matrix[i] = embedding_vector\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First model: Just logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 29, 300)           6977400   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 8700)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 8701      \n",
      "=================================================================\n",
      "Total params: 6,986,101\n",
      "Trainable params: 8,701\n",
      "Non-trainable params: 6,977,400\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 14028 samples, validate on 1559 samples\n",
      "Epoch 1/50\n",
      "14028/14028 [==============================] - 1s 65us/step - loss: 0.4718 - accuracy: 0.7849 - val_loss: 0.4263 - val_accuracy: 0.7960\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.42632, saving model to C:\\Users\\Admin\\Desktop\\CS3244Project\\CNN-kh-n-joyce/model1_postTextweights.01-0.426.hdf5\n",
      "Epoch 2/50\n",
      "14028/14028 [==============================] - 1s 59us/step - loss: 0.4031 - accuracy: 0.8201 - val_loss: 0.4139 - val_accuracy: 0.8031\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.42632 to 0.41391, saving model to C:\\Users\\Admin\\Desktop\\CS3244Project\\CNN-kh-n-joyce/model1_postTextweights.02-0.414.hdf5\n",
      "Epoch 3/50\n",
      "14028/14028 [==============================] - 1s 57us/step - loss: 0.3810 - accuracy: 0.8339 - val_loss: 0.4100 - val_accuracy: 0.8063\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.41391 to 0.41004, saving model to C:\\Users\\Admin\\Desktop\\CS3244Project\\CNN-kh-n-joyce/model1_postTextweights.03-0.410.hdf5\n",
      "Epoch 4/50\n",
      "14028/14028 [==============================] - 1s 56us/step - loss: 0.3673 - accuracy: 0.8402 - val_loss: 0.4068 - val_accuracy: 0.8056\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.41004 to 0.40675, saving model to C:\\Users\\Admin\\Desktop\\CS3244Project\\CNN-kh-n-joyce/model1_postTextweights.04-0.407.hdf5\n",
      "Epoch 5/50\n",
      "14028/14028 [==============================] - 1s 58us/step - loss: 0.3561 - accuracy: 0.8435 - val_loss: 0.4051 - val_accuracy: 0.8108\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.40675 to 0.40511, saving model to C:\\Users\\Admin\\Desktop\\CS3244Project\\CNN-kh-n-joyce/model1_postTextweights.05-0.405.hdf5\n",
      "Epoch 6/50\n",
      "14028/14028 [==============================] - 1s 58us/step - loss: 0.3471 - accuracy: 0.8487 - val_loss: 0.4074 - val_accuracy: 0.8108\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.40511\n",
      "Epoch 7/50\n",
      "14028/14028 [==============================] - 1s 55us/step - loss: 0.3398 - accuracy: 0.8541 - val_loss: 0.4050 - val_accuracy: 0.8133\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.40511 to 0.40497, saving model to C:\\Users\\Admin\\Desktop\\CS3244Project\\CNN-kh-n-joyce/model1_postTextweights.07-0.405.hdf5\n",
      "Epoch 8/50\n",
      "14028/14028 [==============================] - 1s 63us/step - loss: 0.3324 - accuracy: 0.8575 - val_loss: 0.4058 - val_accuracy: 0.8108\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.40497\n",
      "Epoch 9/50\n",
      "14028/14028 [==============================] - 1s 61us/step - loss: 0.3265 - accuracy: 0.8579 - val_loss: 0.4086 - val_accuracy: 0.8127\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.40497\n",
      "Epoch 10/50\n",
      "14028/14028 [==============================] - 1s 57us/step - loss: 0.3210 - accuracy: 0.8638 - val_loss: 0.4078 - val_accuracy: 0.8127\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.40497\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x29f8dbf0f98>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hyper-Params\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "patience = 3\n",
    "\n",
    "#Define model\n",
    "model1 = Sequential()\n",
    "e = Embedding(vocab_size, vec_dim, weights = [embedding_matrix],\n",
    "              input_length = input_length, trainable = False)\n",
    "model1.add(e)\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "model1.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "### summarize the model\n",
    "print(model1.summary())\n",
    "\n",
    "### Create model checkpoint\n",
    "output_dir = cwd\n",
    "modelcheckpoint = ModelCheckpoint(filepath = output_dir + \"/model1_postTextweights.{epoch:02d}-{val_loss:.3f}.hdf5\",\n",
    "                                 monitor='val_loss', verbose=1, save_best_only=True)\n",
    "callbacks_list = [modelcheckpoint, EarlyStopping(monitor = \"val_loss\", patience = patience)]\n",
    "\n",
    "### fit the model\n",
    "model1.fit(padded_X_train, y_train, batch_size = batch_size, epochs=epochs, verbose=1, validation_split = 0.1, callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'accuracy']\n",
      "3897/3897 [==============================] - 0s 40us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5478087533642948, 0.7677700519561768]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Evaluate the model\n",
    "print(model1.metrics_names)\n",
    "model1.evaluate(x=padded_X_test, y=y_test, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second model: Coming soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third model: 1-D convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 29, 300)           6977400   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_5 (Spatial (None, 29, 300)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 29, 100)           90100     \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 29, 100)           30100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 14, 100)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 14, 150)           45150     \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 14, 150)           67650     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 7, 150)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 7, 150)            67650     \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 7, 150)            67650     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 3, 150)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 3, 150)            67650     \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 3, 150)            67650     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_5 (Glob (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 151       \n",
      "=================================================================\n",
      "Total params: 7,481,151\n",
      "Trainable params: 503,751\n",
      "Non-trainable params: 6,977,400\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 14028 samples, validate on 1559 samples\n",
      "Epoch 1/20\n",
      "14028/14028 [==============================] - 10s 694us/step - loss: 0.4569 - accuracy: 0.7934 - val_loss: 0.3945 - val_accuracy: 0.8275\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.39453, saving model to C:\\Users\\Admin\\Desktop\\CS3244Project\\CNN-kh-n-joyce/model3_postTextweights.01-0.395.hdf5\n",
      "Epoch 2/20\n",
      "14028/14028 [==============================] - 9s 663us/step - loss: 0.3921 - accuracy: 0.8318 - val_loss: 0.4309 - val_accuracy: 0.8140\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.39453\n",
      "Epoch 3/20\n",
      "14028/14028 [==============================] - 9s 662us/step - loss: 0.3762 - accuracy: 0.8362 - val_loss: 0.3839 - val_accuracy: 0.8275\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.39453 to 0.38392, saving model to C:\\Users\\Admin\\Desktop\\CS3244Project\\CNN-kh-n-joyce/model3_postTextweights.03-0.384.hdf5\n",
      "Epoch 4/20\n",
      "14028/14028 [==============================] - 9s 645us/step - loss: 0.3644 - accuracy: 0.8428 - val_loss: 0.3685 - val_accuracy: 0.8416\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.38392 to 0.36854, saving model to C:\\Users\\Admin\\Desktop\\CS3244Project\\CNN-kh-n-joyce/model3_postTextweights.04-0.369.hdf5\n",
      "Epoch 5/20\n",
      "14028/14028 [==============================] - 9s 649us/step - loss: 0.3501 - accuracy: 0.8462 - val_loss: 0.3844 - val_accuracy: 0.8281\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.36854\n",
      "Epoch 6/20\n",
      "14028/14028 [==============================] - 9s 642us/step - loss: 0.3428 - accuracy: 0.8509 - val_loss: 0.3797 - val_accuracy: 0.8268\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.36854\n",
      "Epoch 7/20\n",
      "14028/14028 [==============================] - 9s 648us/step - loss: 0.3301 - accuracy: 0.8571 - val_loss: 0.4013 - val_accuracy: 0.8262\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.36854\n",
      "Epoch 8/20\n",
      "14028/14028 [==============================] - 9s 661us/step - loss: 0.3188 - accuracy: 0.8630 - val_loss: 0.4076 - val_accuracy: 0.8249\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.36854\n",
      "Epoch 9/20\n",
      "14028/14028 [==============================] - 9s 664us/step - loss: 0.3054 - accuracy: 0.8680 - val_loss: 0.3966 - val_accuracy: 0.8198\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.36854\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x29fc616d630>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hyper-Params\n",
    "epochs = 20\n",
    "batch_size = 32 #for stochastic gradient descent\n",
    "drop_embed = 0.5\n",
    "\n",
    "n_dense = 256\n",
    "dropout = 0.5\n",
    "\n",
    "n_conv_layer1 = 100\n",
    "n_conv_layer2 = 150\n",
    "n_conv_layer3 = 150\n",
    "n_conv_layer4 = 150\n",
    "#n_conv_layer5 = 100\n",
    "k_conv = 3\n",
    "\n",
    "#Define model\n",
    "\n",
    "model3 = Sequential()\n",
    "e = Embedding(vocab_size, vec_dim, weights = [embedding_matrix],\n",
    "              input_length = input_length, trainable = False)\n",
    "model3.add(e)\n",
    "model3.add(SpatialDropout1D(drop_embed))\n",
    "model3.add(Conv1D(filters = n_conv_layer1, kernel_size = k_conv, activation = \"relu\", padding = \"same\"))\n",
    "model3.add(Conv1D(filters = n_conv_layer1, kernel_size = k_conv, activation = \"relu\", padding = \"same\"))\n",
    "model3.add(MaxPooling1D(2))\n",
    "model3.add(Conv1D(filters = n_conv_layer2, kernel_size = k_conv, activation = \"relu\", padding = \"same\"))\n",
    "model3.add(Conv1D(filters = n_conv_layer2, kernel_size = k_conv, activation = \"relu\", padding = \"same\"))\n",
    "model3.add(MaxPooling1D(2))\n",
    "model3.add(Conv1D(filters = n_conv_layer3, kernel_size = k_conv, activation = \"relu\", padding = \"same\"))\n",
    "model3.add(Conv1D(filters = n_conv_layer3, kernel_size = k_conv, activation = \"relu\", padding = \"same\"))\n",
    "model3.add(MaxPooling1D(2))\n",
    "model3.add(Conv1D(filters = n_conv_layer4, kernel_size = k_conv, activation = \"relu\", padding = \"same\"))\n",
    "model3.add(Conv1D(filters = n_conv_layer4, kernel_size = k_conv, activation = \"relu\", padding = \"same\"))\n",
    "#model3.add(MaxPooling1D(2))\n",
    "#model3.add(Conv1D(filters = n_conv_layer5, kernel_size = k_conv, activation = \"relu\", padding = \"same\"))\n",
    "#model3.add(Conv1D(filters = n_conv_layer5, kernel_size = k_conv, activation = \"relu\", padding = \"same\"))\n",
    "model3.add(GlobalMaxPool1D())\n",
    "model3.add(Dropout(dropout))\n",
    "#model3.add(Dense(n_dense, activation = \"relu\"))\n",
    "#model3.add(Dropout(dropout))\n",
    "model3.add(Dense(1, activation = \"sigmoid\"))\n",
    "model3.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "#              loss = \"mean_squared_error\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "### summarize the model\n",
    "print(model3.summary())\n",
    "\n",
    "### Create model checkpoint\n",
    "output_dir = cwd\n",
    "modelcheckpoint = ModelCheckpoint(filepath = output_dir + \"/model3_postTextweights.{epoch:02d}-{val_loss:.3f}.hdf5\",\n",
    "                                 monitor='val_loss', verbose=1, save_best_only=True)\n",
    "callbacks_list = [modelcheckpoint, EarlyStopping(monitor = \"val_loss\", patience = 5)]\n",
    "\n",
    "#callbacks_list_no_chkpt = [EarlyStopping(monitor = \"val_loss\", patience = 2)]\n",
    "\n",
    "### fit the model\n",
    "model3.fit(padded_X_train, y_train, batch_size = batch_size, epochs=epochs, verbose=1, validation_split = 0.1, callbacks = callbacks_list)\n",
    "\n",
    "#fit with no checkpointing\n",
    "#model3.fit(padded_X_train, y_train, batch_size = batch_size, epochs=epochs, verbose=1, validation_split = 0.1, callbacks = callbacks_list_no_chkpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'accuracy']\n",
      "3897/3897 [==============================] - 1s 253us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5324315230816427, 0.7836797833442688]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Evaluate the best model\n",
    "model3_loaded = load_model(\"model3_postTextweights.01-0.359.hdf5\")\n",
    "print(model3_loaded.metrics_names)\n",
    "model3_loaded.evaluate(x=padded_X_test, y=y_test, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
