{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from load_dataset import get_dataset\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import FastText as ft\n",
    "\n",
    "from keras import layers, models, optimizers\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D,MaxPooling1D,Dropout,GlobalMaxPool1D,SpatialDropout1D\n",
    "\n",
    "\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python 3.6.4\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `load_fasttext_format` (use load_facebook_vectors (to use pretrained embeddings) or load_facebook_model (to continue training with the loaded full model, more RAM) instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "c:\\python 3.6.4\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "embed_model2 = ft.load_fasttext_format(\"cc.en.300.bin\")\n",
    "\n",
    "vec_dim = len(embed_model2[\"cat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_dataset(size = \"small\") #use small for small dataset, large for large dataset. Will print the number of articles loaded.\n",
    "\n",
    "X = df[\"content\"]\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = 0.2\n",
    "\n",
    "# (random_state): we use a fixed random seed so we get the same results every time.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = percentage, random_state=51)\n",
    "\n",
    "##Finding length of longest article\n",
    "##Yes this is like \"cheating\" because I'm considering the test data too but I have no choice here...\n",
    "\n",
    "def get_max_length():\n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(X)\n",
    "    encoded_X = t.texts_to_sequences(X)\n",
    "    max_length = 0\n",
    "    for i in encoded_X:\n",
    "        max_length = max(len(i), max_length)\n",
    "    \n",
    "    return max_length + 1\n",
    "\n",
    "input_length = get_max_length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_input(array):\n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(array)\n",
    "    vocab_size = len(t.word_index) + 1\n",
    "    encoded_X = t.texts_to_sequences(array)\n",
    "\n",
    "    ##input_length is a global variable\n",
    "    padded_X = pad_sequences(encoded_X, maxlen = input_length,\n",
    "                         padding = \"post\")\n",
    "    \n",
    "    return (padded_X, t)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python 3.6.4\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Create the embedding matrix using the training data, as well as the padded X_train data\n",
    "padded_X_train, t_train = pre_process_input(X_train)\n",
    "vocab_size = len(t_train.word_index) + 1\n",
    "input_length = len(padded_X_train[0])\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, vec_dim))\n",
    "for word,i in t_train.word_index.items():\n",
    "    embedding_vector = embed_model2[word]\n",
    "    embedding_matrix[i] = embedding_vector\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the padded X_test data\n",
    "padded_X_test = pre_process_input(X_test)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First model: Just logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 5847, 300)         14930100  \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1754100)           0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 1754101   \n",
      "=================================================================\n",
      "Total params: 16,684,201\n",
      "Trainable params: 1,754,101\n",
      "Non-trainable params: 14,930,100\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1573 samples, validate on 394 samples\n",
      "Epoch 1/10\n",
      "1573/1573 [==============================] - 6s 4ms/step - loss: 1.0147 - accuracy: 0.6662 - val_loss: 0.7173 - val_accuracy: 0.6827\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.71731, saving model to C:\\Users\\Admin\\Desktop\\CS3244Project\\CNN-kh-n-joyce/model1weights.01-0.72.hdf5\n",
      "Epoch 2/10\n",
      "1573/1573 [==============================] - 6s 4ms/step - loss: 0.4993 - accuracy: 0.9161 - val_loss: 0.7732 - val_accuracy: 0.6929\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.71731\n",
      "Epoch 3/10\n",
      "1573/1573 [==============================] - 6s 4ms/step - loss: 0.5215 - accuracy: 0.9555 - val_loss: 0.8109 - val_accuracy: 0.6954\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.71731\n",
      "Epoch 4/10\n",
      "1573/1573 [==============================] - 6s 4ms/step - loss: 0.5159 - accuracy: 0.9574 - val_loss: 0.8540 - val_accuracy: 0.6929\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.71731\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x296aee84748>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hyper-Params\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "patience = 3\n",
    "\n",
    "#Define model\n",
    "model1 = Sequential()\n",
    "e = Embedding(vocab_size, vec_dim, weights = [embedding_matrix],\n",
    "              input_length = input_length, trainable = False)\n",
    "model1.add(e)\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "model1.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "### summarize the model\n",
    "print(model1.summary())\n",
    "\n",
    "### Create model checkpoint\n",
    "output_dir = cwd\n",
    "modelcheckpoint = ModelCheckpoint(filepath = output_dir + \"/model1weights.{epoch:02d}-{val_loss:.2f}.hdf5\",\n",
    "                                 monitor='val_loss', verbose=1, save_best_only=True)\n",
    "callbacks_list = [modelcheckpoint, EarlyStopping(monitor = \"val_loss\", patience = patience)]\n",
    "\n",
    "### fit the model\n",
    "model1.fit(padded_X_train, y_train, batch_size = batch_size, epochs=epochs, verbose=1, validation_split = 0.2, callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'accuracy']\n",
      "492/492 [==============================] - 1s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8104413229275526, 0.6463414430618286]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Evaluate the model\n",
    "print(model1.metrics_names)\n",
    "model1.evaluate(x=padded_X_test, y=y_test, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second model: Coming soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third model: 1-D convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 5847, 300)         14930100  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_2 (Spatial (None, 5847, 300)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 5847, 100)         150100    \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 5847, 100)         50100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 1949, 100)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 1949, 100)         50100     \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 1949, 100)         50100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 649, 100)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 649, 160)          80160     \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 649, 160)          128160    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 15,438,981\n",
      "Trainable params: 508,881\n",
      "Non-trainable params: 14,930,100\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1573 samples, validate on 394 samples\n",
      "Epoch 1/10\n",
      "1573/1573 [==============================] - 106s 67ms/step - loss: 0.6297 - accuracy: 0.6815 - val_loss: 0.6183 - val_accuracy: 0.6726\n",
      "Epoch 2/10\n",
      "1573/1573 [==============================] - 105s 67ms/step - loss: 0.6110 - accuracy: 0.6866 - val_loss: 0.6023 - val_accuracy: 0.6726\n",
      "Epoch 3/10\n",
      "1573/1573 [==============================] - 108s 68ms/step - loss: 0.6058 - accuracy: 0.6866 - val_loss: 0.6381 - val_accuracy: 0.6726\n",
      "Epoch 4/10\n",
      "1573/1573 [==============================] - 109s 69ms/step - loss: 0.5839 - accuracy: 0.6968 - val_loss: 0.6121 - val_accuracy: 0.6726\n",
      "Epoch 5/10\n",
      "1573/1573 [==============================] - 110s 70ms/step - loss: 0.5607 - accuracy: 0.6910 - val_loss: 0.6037 - val_accuracy: 0.6574\n",
      "Epoch 6/10\n",
      "1573/1573 [==============================] - 110s 70ms/step - loss: 0.5430 - accuracy: 0.7190 - val_loss: 0.6397 - val_accuracy: 0.6904\n",
      "Epoch 7/10\n",
      "1573/1573 [==============================] - 110s 70ms/step - loss: 0.5151 - accuracy: 0.7247 - val_loss: 0.6385 - val_accuracy: 0.6726\n",
      "Epoch 8/10\n",
      "1573/1573 [==============================] - 111s 70ms/step - loss: 0.4711 - accuracy: 0.7711 - val_loss: 0.7510 - val_accuracy: 0.6954\n",
      "Epoch 9/10\n",
      "1573/1573 [==============================] - 110s 70ms/step - loss: 0.4432 - accuracy: 0.7877 - val_loss: 0.6393 - val_accuracy: 0.6827\n",
      "Epoch 10/10\n",
      "1573/1573 [==============================] - 110s 70ms/step - loss: 0.4109 - accuracy: 0.8017 - val_loss: 0.7382 - val_accuracy: 0.6726\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x296b5fb5128>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hyper-Params\n",
    "epochs = 10\n",
    "batch_size = 32 #for stochastic gradient descent\n",
    "drop_embed = 0.1\n",
    "\n",
    "n_dense = 256\n",
    "dropout = 0.4\n",
    "\n",
    "n_conv_layer1 = 100\n",
    "n_conv_layer2 = 160\n",
    "k_conv = 5\n",
    "\n",
    "#Define model\n",
    "\n",
    "model3 = Sequential()\n",
    "e = Embedding(vocab_size, vec_dim, weights = [embedding_matrix],\n",
    "              input_length = input_length, trainable = False)\n",
    "model3.add(e)\n",
    "model3.add(SpatialDropout1D(drop_embed))\n",
    "model3.add(Conv1D(filters = n_conv_layer1, kernel_size = k_conv, activation = \"relu\", padding = \"same\"))\n",
    "model3.add(Conv1D(filters = n_conv_layer1, kernel_size = k_conv, activation = \"relu\", padding = \"same\"))\n",
    "model3.add(MaxPooling1D(3))\n",
    "model3.add(Conv1D(filters = n_conv_layer1, kernel_size = k_conv, activation = \"relu\", padding = \"same\"))\n",
    "model3.add(Conv1D(filters = n_conv_layer1, kernel_size = k_conv, activation = \"relu\", padding = \"same\"))\n",
    "model3.add(MaxPooling1D(3))\n",
    "model3.add(Conv1D(filters = n_conv_layer2, kernel_size = k_conv, activation = \"relu\", padding = \"same\"))\n",
    "model3.add(Conv1D(filters = n_conv_layer2, kernel_size = k_conv, activation = \"relu\", padding = \"same\"))\n",
    "model3.add(GlobalMaxPool1D())\n",
    "#model3.add(Dense(n_dense, activation = \"relu\"))\n",
    "model3.add(Dropout(dropout))\n",
    "model3.add(Dense(1, activation = \"sigmoid\"))\n",
    "model3.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "### summarize the model\n",
    "print(model3.summary())\n",
    "\n",
    "### Create model checkpoint\n",
    "output_dir = cwd\n",
    "modelcheckpoint = ModelCheckpoint(filepath = output_dir + \"/model3weights.{epoch:02d}-{val_loss:.2f}.hdf5\",\n",
    "                                 monitor='val_loss', verbose=1, save_best_only=True)\n",
    "callbacks_list = [modelcheckpoint, EarlyStopping(monitor = \"val_loss\", patience = 5)]\n",
    "\n",
    "### fit the model\n",
    "#model3.fit(padded_X_train, y_train, batch_size = batch_size, epochs=epochs, verbose=1, validation_split = 0.2, callbacks = callbacks_list)\n",
    "\n",
    "#fit with no checkpointing\n",
    "model3.fit(padded_X_train, y_train, batch_size = batch_size, epochs=epochs, verbose=1, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'accuracy']\n",
      "492/492 [==============================] - 11s 22ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7192409256609474, 0.565040647983551]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Evaluate the model\n",
    "print(model3.metrics_names)\n",
    "model3.evaluate(x=padded_X_test, y=y_test, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
