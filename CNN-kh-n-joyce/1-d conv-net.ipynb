{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import FastText as ft\n",
    "\n",
    "from keras import layers, models, optimizers\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D,MaxPooling1D,Dropout,GlobalMaxPool1D,SpatialDropout1D\n",
    "from small_dataset import get_small_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to convert glove data to word2vec data\n",
    "#from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "#glove_input_file = 'glove.840B.300d.txt'\n",
    "#word2vec_output_file = 'glove.840B.300d.txt.word2vec'\n",
    "#glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_small_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"content\"]\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This model has lots of missing vocab\n",
    "#But this model is taken from news articles, which probably fits the clickbait titles well.\n",
    "# filename = 'GoogleNews-vectors-negative300.bin'\n",
    "# embed_model = KeyedVectors.load_word2vec_format(filename, binary=True) \n",
    "\n",
    "#This is now depreciated, we will use fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python 3.6.4\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `load_fasttext_format` (use load_facebook_vectors (to use pretrained embeddings) or load_facebook_model (to continue training with the loaded full model, more RAM) instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "embed_model2 = ft.load_fasttext_format(\"cc.en.300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python 3.6.4\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "vec_dim = len(embed_model2[\"cat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-process the input matrix X\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(X)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "encoded_X = t.texts_to_sequences(X)\n",
    "\n",
    "max_length = 0\n",
    "for i in encoded_X:\n",
    "    max_length = max(len(i), max_length)\n",
    "    \n",
    "input_length = max_length + 1\n",
    "\n",
    "padded_X = pad_sequences(encoded_X, maxlen = input_length,\n",
    "                         padding = \"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python 3.6.4\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "#Create the embedding matrix\n",
    "#If the word is not in the vocab, then let it take a vector of all zeros\n",
    "#Not the best idea, given that so many words are out of the vocab...\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, vec_dim))\n",
    "for word,i in t.word_index.items():\n",
    "    embedding_vector = embed_model2[word]\n",
    "    embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here we split the training data into training and validation, and print some general statistics about the training data\n",
    "\"\"\"\n",
    "# percentage of validation data\n",
    "# percentage = 0.2\n",
    "\n",
    "# (random_state): we use a fixed random seed so we get the same results every time.\n",
    "# X_train, X_val, y_train, y_val = train_test_split(padded_X, y, test_size = percentage, random_state=51)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First model: Just logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_19 (Embedding)     (None, 5847, 300)         16605900  \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 1754100)           0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 1)                 1754101   \n",
      "=================================================================\n",
      "Total params: 18,360,001\n",
      "Trainable params: 1,754,101\n",
      "Non-trainable params: 16,605,900\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1967 samples, validate on 492 samples\n",
      "Epoch 1/10\n",
      "1967/1967 [==============================] - 7s 4ms/step - loss: 0.9821 - accuracy: 0.6721 - val_loss: 1.2209 - val_accuracy: 0.7154\n",
      "Epoch 2/10\n",
      "1967/1967 [==============================] - 7s 4ms/step - loss: 0.3874 - accuracy: 0.9136 - val_loss: 1.1808 - val_accuracy: 0.7358\n",
      "Epoch 3/10\n",
      "1967/1967 [==============================] - 7s 4ms/step - loss: 0.5519 - accuracy: 0.9497 - val_loss: 1.4209 - val_accuracy: 0.6870\n",
      "Epoch 4/10\n",
      "1967/1967 [==============================] - 7s 4ms/step - loss: 0.5135 - accuracy: 0.9573 - val_loss: 1.5346 - val_accuracy: 0.7093\n",
      "Epoch 5/10\n",
      "1967/1967 [==============================] - 7s 4ms/step - loss: 0.5170 - accuracy: 0.9614 - val_loss: 1.2826 - val_accuracy: 0.7276\n",
      "Epoch 6/10\n",
      "1967/1967 [==============================] - 7s 4ms/step - loss: 0.3905 - accuracy: 0.9624 - val_loss: 1.3003 - val_accuracy: 0.7114\n",
      "Epoch 7/10\n",
      "1967/1967 [==============================] - 7s 4ms/step - loss: 0.2999 - accuracy: 0.9624 - val_loss: 1.2097 - val_accuracy: 0.7154\n",
      "Epoch 8/10\n",
      "1967/1967 [==============================] - 7s 4ms/step - loss: 0.7547 - accuracy: 0.9603 - val_loss: 1.4058 - val_accuracy: 0.7154\n",
      "Epoch 9/10\n",
      "1967/1967 [==============================] - 7s 4ms/step - loss: 0.6378 - accuracy: 0.9614 - val_loss: 1.7745 - val_accuracy: 0.7093\n",
      "Epoch 10/10\n",
      "1967/1967 [==============================] - 7s 4ms/step - loss: 0.3021 - accuracy: 0.9609 - val_loss: 1.4331 - val_accuracy: 0.7175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a073861f28>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define model\n",
    "\n",
    "model1 = Sequential()\n",
    "e = Embedding(vocab_size, vec_dim, weights = [embedding_matrix],\n",
    "              input_length = input_length, trainable = False)\n",
    "model1.add(e)\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "model1.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "### summarize the model\n",
    "print(model1.summary())\n",
    "\n",
    "### fit the model\n",
    "model1.fit(padded_X, y, epochs=10, verbose=1, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second model: Neural network with 3 hidden layers, with little regularisation but with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_42\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_42 (Embedding)     (None, 5847, 300)         16605900  \n",
      "_________________________________________________________________\n",
      "flatten_28 (Flatten)         (None, 1754100)           0         \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 1754100)           0         \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 100)               175410100 \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 192,036,301\n",
      "Trainable params: 175,430,401\n",
      "Non-trainable params: 16,605,900\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1967 samples, validate on 492 samples\n",
      "Epoch 1/50\n",
      "1967/1967 [==============================] - 170s 87ms/step - loss: 1.2917 - accuracy: 0.6604 - val_loss: 1.3856 - val_accuracy: 0.6992\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.38565, saving model to D:\\OneDrive - National University of Singapore\\NUS STUFF\\Y4 Sem 1\\CS3244\\CS3244Project\\Scripts\\CNN-autoencoder/model2weights.01-1.39.hdf5\n",
      "Epoch 2/50\n",
      "1967/1967 [==============================] - 168s 85ms/step - loss: 1.1592 - accuracy: 0.8480 - val_loss: 1.5997 - val_accuracy: 0.7053\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.38565\n",
      "Epoch 3/50\n",
      "1967/1967 [==============================] - 172s 87ms/step - loss: 1.0074 - accuracy: 0.9436 - val_loss: 1.8977 - val_accuracy: 0.7012\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.38565\n",
      "Epoch 4/50\n",
      "1967/1967 [==============================] - 172s 87ms/step - loss: 1.0980 - accuracy: 0.9548 - val_loss: 1.5021 - val_accuracy: 0.6931\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.38565\n",
      "Epoch 5/50\n",
      "1967/1967 [==============================] - 174s 88ms/step - loss: 0.9118 - accuracy: 0.9537 - val_loss: 1.6523 - val_accuracy: 0.7012\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.38565\n",
      "Epoch 6/50\n",
      "1967/1967 [==============================] - 174s 89ms/step - loss: 0.7799 - accuracy: 0.9542 - val_loss: 1.7925 - val_accuracy: 0.6992\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.38565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x19f8405cf98>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hyper-Params\n",
    "epochs = 50\n",
    "batch_size = 32 #for stochastic gradient descent\n",
    "n_dense = 100\n",
    "dropout = 0.5\n",
    "\n",
    "\n",
    "#Define model\n",
    "\n",
    "model2 = Sequential()\n",
    "e = Embedding(vocab_size, vec_dim, weights = [embedding_matrix],\n",
    "              input_length = input_length, trainable = False)\n",
    "model2.add(e)\n",
    "model2.add(Flatten())\n",
    "model2.add(Dropout(dropout))\n",
    "model2.add(Dense(n_dense, kernel_regularizer=l2(0.0005), activation = \"relu\"))\n",
    "model2.add(Dense(n_dense, kernel_regularizer=l2(0.0005), activation = \"relu\"))\n",
    "model2.add(Dropout(dropout/2))\n",
    "model2.add(Dense(n_dense, kernel_regularizer=l2(0.0005), activation = \"relu\"))\n",
    "model2.add(Dense(1, activation = \"sigmoid\"))\n",
    "model2.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "### summarize the model\n",
    "print(model2.summary())\n",
    "\n",
    "### Create model checkpoint\n",
    "output_dir = cwd\n",
    "modelcheckpoint = ModelCheckpoint(filepath = output_dir + \"/model2weights.{epoch:02d}-{val_loss:.2f}.hdf5\",\n",
    "                                 monitor='val_loss', verbose=1, save_best_only=True)\n",
    "callbacks_list = [modelcheckpoint, EarlyStopping(monitor = \"val_loss\", patience = 5)]\n",
    "\n",
    "### fit the model\n",
    "model2.fit(padded_X, y, batch_size = batch_size, epochs=epochs, verbose=1, validation_split = 0.2, callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third model: 1-D convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_38\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_38 (Embedding)     (None, 5847, 300)         16605900  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_13 (Spatia (None, 5847, 300)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 5847, 100)         150100    \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 5847, 100)         50100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1949, 100)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 1949, 100)         50100     \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 1949, 100)         50100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 649, 100)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 649, 160)          80160     \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 649, 160)          128160    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_13 (Glo (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 17,114,781\n",
      "Trainable params: 508,881\n",
      "Non-trainable params: 16,605,900\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1967 samples, validate on 492 samples\n",
      "Epoch 1/250\n",
      "1967/1967 [==============================] - 140s 71ms/step - loss: 0.6343 - accuracy: 0.6894 - val_loss: 0.6102 - val_accuracy: 0.6850\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.61020, saving model to D:\\OneDrive - National University of Singapore\\NUS STUFF\\Y4 Sem 1\\CS3244\\CS3244Project\\Scripts\\CNN-autoencoder/model3weights.01-0.61.hdf5\n",
      "Epoch 2/250\n",
      "1967/1967 [==============================] - 144s 73ms/step - loss: 0.6081 - accuracy: 0.6914 - val_loss: 0.5888 - val_accuracy: 0.6850\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.61020 to 0.58883, saving model to D:\\OneDrive - National University of Singapore\\NUS STUFF\\Y4 Sem 1\\CS3244\\CS3244Project\\Scripts\\CNN-autoencoder/model3weights.02-0.59.hdf5\n",
      "Epoch 3/250\n",
      "1967/1967 [==============================] - 144s 73ms/step - loss: 0.5919 - accuracy: 0.6919 - val_loss: 0.5713 - val_accuracy: 0.6850\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.58883 to 0.57132, saving model to D:\\OneDrive - National University of Singapore\\NUS STUFF\\Y4 Sem 1\\CS3244\\CS3244Project\\Scripts\\CNN-autoencoder/model3weights.03-0.57.hdf5\n",
      "Epoch 4/250\n",
      "1967/1967 [==============================] - 144s 73ms/step - loss: 0.5765 - accuracy: 0.6980 - val_loss: 0.5547 - val_accuracy: 0.7236\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.57132 to 0.55475, saving model to D:\\OneDrive - National University of Singapore\\NUS STUFF\\Y4 Sem 1\\CS3244\\CS3244Project\\Scripts\\CNN-autoencoder/model3weights.04-0.55.hdf5\n",
      "Epoch 5/250\n",
      "1967/1967 [==============================] - 146s 74ms/step - loss: 0.5514 - accuracy: 0.7082 - val_loss: 0.5654 - val_accuracy: 0.7114\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.55475\n",
      "Epoch 6/250\n",
      "1967/1967 [==============================] - 146s 74ms/step - loss: 0.5190 - accuracy: 0.7407 - val_loss: 0.5622 - val_accuracy: 0.7317\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.55475\n",
      "Epoch 7/250\n",
      "1967/1967 [==============================] - 145s 74ms/step - loss: 0.5187 - accuracy: 0.7382 - val_loss: 0.5677 - val_accuracy: 0.7154\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.55475\n",
      "Epoch 8/250\n",
      "1967/1967 [==============================] - 145s 74ms/step - loss: 0.4633 - accuracy: 0.7733 - val_loss: 0.5563 - val_accuracy: 0.7114\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.55475\n",
      "Epoch 9/250\n",
      "1967/1967 [==============================] - 144s 73ms/step - loss: 0.4367 - accuracy: 0.7870 - val_loss: 0.6078 - val_accuracy: 0.6260\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.55475\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a0ee9e4fd0>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hyper-Params\n",
    "epochs = 250\n",
    "batch_size = 32 #for stochastic gradient descent\n",
    "drop_embed = 0.1\n",
    "\n",
    "n_dense = 256\n",
    "dropout = 0.4\n",
    "\n",
    "n_conv_layer1 = 100\n",
    "n_conv_layer2 = 160\n",
    "k_conv = 5\n",
    "\n",
    "#Define model\n",
    "\n",
    "model3 = Sequential()\n",
    "e = Embedding(vocab_size, vec_dim, weights = [embedding_matrix],\n",
    "              input_length = input_length, trainable = False)\n",
    "model3.add(e)\n",
    "model3.add(SpatialDropout1D(drop_embed))\n",
    "model3.add(Conv1D(filters = n_conv_layer1, kernel_size = k_conv, activation = \"relu\", padding = \"same\"))\n",
    "model3.add(Conv1D(filters = n_conv_layer1, kernel_size = k_conv, activation = \"relu\", padding = \"same\"))\n",
    "model3.add(MaxPooling1D(3))\n",
    "model3.add(Conv1D(filters = n_conv_layer1, kernel_size = k_conv, activation = \"relu\", padding = \"same\"))\n",
    "model3.add(Conv1D(filters = n_conv_layer1, kernel_size = k_conv, activation = \"relu\", padding = \"same\"))\n",
    "model3.add(MaxPooling1D(3))\n",
    "model3.add(Conv1D(filters = n_conv_layer2, kernel_size = k_conv, activation = \"relu\", padding = \"same\"))\n",
    "model3.add(Conv1D(filters = n_conv_layer2, kernel_size = k_conv, activation = \"relu\", padding = \"same\"))\n",
    "model3.add(GlobalMaxPool1D())\n",
    "#model3.add(Dense(n_dense, activation = \"relu\"))\n",
    "model3.add(Dropout(dropout))\n",
    "model3.add(Dense(1, activation = \"sigmoid\"))\n",
    "model3.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "### summarize the model\n",
    "print(model3.summary())\n",
    "\n",
    "### Create model checkpoint\n",
    "output_dir = cwd\n",
    "modelcheckpoint = ModelCheckpoint(filepath = output_dir + \"/model3weights.{epoch:02d}-{val_loss:.2f}.hdf5\",\n",
    "                                 monitor='val_loss', verbose=1, save_best_only=True)\n",
    "callbacks_list = [modelcheckpoint, EarlyStopping(monitor = \"val_loss\", patience = 5)]\n",
    "\n",
    "### fit the model\n",
    "model3.fit(padded_X, y, batch_size = batch_size, epochs=epochs, verbose=1, validation_split = 0.2, callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
