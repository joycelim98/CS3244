{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from load_dataset_title import get_dataset\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import FastText as ft\n",
    "\n",
    "from keras import layers, models, optimizers\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D,MaxPooling1D,Dropout,GlobalMaxPool1D,SpatialDropout1D,AveragePooling1D,GlobalAveragePooling1D\n",
    "\n",
    "\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python 3.6.4\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `load_fasttext_format` (use load_facebook_vectors (to use pretrained embeddings) or load_facebook_model (to continue training with the loaded full model, more RAM) instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "c:\\python 3.6.4\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "embed_model2 = ft.load_fasttext_format(\"cc.en.300.bin\")\n",
    "\n",
    "vec_dim = len(embed_model2[\"cat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading large document\n",
      "19539\n"
     ]
    }
   ],
   "source": [
    "df = get_dataset(size = \"large\") #use small for small dataset, large for large dataset. Will print the number of articles loaded.\n",
    "\n",
    "X = df[\"title\"]\n",
    "y = df['label']\n",
    "\n",
    "percentage = 0.2\n",
    "\n",
    "# (random_state): we use a fixed random seed so we get the same results every time.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = percentage, random_state=51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "##Finding length of longest article\n",
    "\n",
    "def get_max_length():\n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(X)\n",
    "    encoded_X = t.texts_to_sequences(X)\n",
    "    max_length = 0\n",
    "    for i in encoded_X:\n",
    "        max_length = max(len(i), max_length)\n",
    "    \n",
    "    return max_length + 1\n",
    "\n",
    "input_length = get_max_length()\n",
    "print(input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_input(array):\n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(array)\n",
    "    vocab_size = len(t.word_index) + 1\n",
    "    encoded_X = t.texts_to_sequences(array)\n",
    "\n",
    "    ##input_length is a global variable\n",
    "    padded_X = pad_sequences(encoded_X, maxlen = input_length,\n",
    "                         padding = \"post\")\n",
    "    \n",
    "    return (padded_X, t)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python 3.6.4\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Create the embedding matrix using the training data, as well as the padded X_train data\n",
    "padded_X_train, t_train = pre_process_input(X_train)\n",
    "vocab_size = len(t_train.word_index) + 1\n",
    "input_length = len(padded_X_train[0])\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, vec_dim))\n",
    "for word,i in t_train.word_index.items():\n",
    "    embedding_vector = embed_model2[word]\n",
    "    embedding_matrix[i] = embedding_vector\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the padded X_test data\n",
    "padded_X_test = pre_process_input(X_test)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First model: Just logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_33 (Embedding)     (None, 31, 300)           8593500   \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 9300)              0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 1)                 9301      \n",
      "=================================================================\n",
      "Total params: 8,602,801\n",
      "Trainable params: 9,301\n",
      "Non-trainable params: 8,593,500\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 12504 samples, validate on 3126 samples\n",
      "Epoch 1/50\n",
      "12504/12504 [==============================] - 1s 83us/step - loss: 0.4969 - accuracy: 0.7718 - val_loss: 0.4449 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.44488, saving model to C:\\Users\\Admin\\Desktop\\CS3244Project\\CNN-kh-n-joyce/model1_postTextweights.01-0.445.hdf5\n",
      "Epoch 2/50\n",
      "12504/12504 [==============================] - 1s 80us/step - loss: 0.4264 - accuracy: 0.8045 - val_loss: 0.4225 - val_accuracy: 0.8119\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.44488 to 0.42249, saving model to C:\\Users\\Admin\\Desktop\\CS3244Project\\CNN-kh-n-joyce/model1_postTextweights.02-0.422.hdf5\n",
      "Epoch 3/50\n",
      "12504/12504 [==============================] - 1s 73us/step - loss: 0.4003 - accuracy: 0.8157 - val_loss: 0.4131 - val_accuracy: 0.8170\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.42249 to 0.41315, saving model to C:\\Users\\Admin\\Desktop\\CS3244Project\\CNN-kh-n-joyce/model1_postTextweights.03-0.413.hdf5\n",
      "Epoch 4/50\n",
      "12504/12504 [==============================] - 1s 75us/step - loss: 0.3837 - accuracy: 0.8263 - val_loss: 0.4110 - val_accuracy: 0.8164\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.41315 to 0.41104, saving model to C:\\Users\\Admin\\Desktop\\CS3244Project\\CNN-kh-n-joyce/model1_postTextweights.04-0.411.hdf5\n",
      "Epoch 5/50\n",
      "12504/12504 [==============================] - 1s 75us/step - loss: 0.3713 - accuracy: 0.8321 - val_loss: 0.4083 - val_accuracy: 0.8202\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.41104 to 0.40828, saving model to C:\\Users\\Admin\\Desktop\\CS3244Project\\CNN-kh-n-joyce/model1_postTextweights.05-0.408.hdf5\n",
      "Epoch 6/50\n",
      "12504/12504 [==============================] - 1s 74us/step - loss: 0.3614 - accuracy: 0.8376 - val_loss: 0.4085 - val_accuracy: 0.8167\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.40828\n",
      "Epoch 7/50\n",
      "12504/12504 [==============================] - 1s 75us/step - loss: 0.3529 - accuracy: 0.8445 - val_loss: 0.4056 - val_accuracy: 0.8225\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.40828 to 0.40565, saving model to C:\\Users\\Admin\\Desktop\\CS3244Project\\CNN-kh-n-joyce/model1_postTextweights.07-0.406.hdf5\n",
      "Epoch 8/50\n",
      "12504/12504 [==============================] - 1s 75us/step - loss: 0.3454 - accuracy: 0.8479 - val_loss: 0.4078 - val_accuracy: 0.8186\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.40565\n",
      "Epoch 9/50\n",
      "12504/12504 [==============================] - 1s 72us/step - loss: 0.3391 - accuracy: 0.8508 - val_loss: 0.4064 - val_accuracy: 0.8221\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.40565\n",
      "Epoch 10/50\n",
      "12504/12504 [==============================] - 1s 72us/step - loss: 0.3336 - accuracy: 0.8543 - val_loss: 0.4079 - val_accuracy: 0.8221\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.40565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x16f2a215fd0>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hyper-Params\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "patience = 3\n",
    "\n",
    "#Define model\n",
    "model1 = Sequential()\n",
    "e = Embedding(vocab_size, vec_dim, weights = [embedding_matrix],\n",
    "              input_length = input_length, trainable = False)\n",
    "model1.add(e)\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "model1.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "### summarize the model\n",
    "print(model1.summary())\n",
    "\n",
    "### Create model checkpoint\n",
    "output_dir = cwd\n",
    "modelcheckpoint = ModelCheckpoint(filepath = output_dir + \"/model1_postTextweights.{epoch:02d}-{val_loss:.3f}.hdf5\",\n",
    "                                 monitor='val_loss', verbose=1, save_best_only=True)\n",
    "callbacks_list = [modelcheckpoint, EarlyStopping(monitor = \"val_loss\", patience = patience)]\n",
    "\n",
    "### fit the model\n",
    "model1.fit(padded_X_train, y_train, batch_size = batch_size, epochs=epochs, verbose=1, validation_split = 0.2, callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'accuracy']\n",
      "3908/3908 [==============================] - 0s 46us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5446823031289795, 0.7725179195404053]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Evaluate the model\n",
    "print(model1.metrics_names)\n",
    "model1.evaluate(x=padded_X_test, y=y_test, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second model: Coming soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third model: 1-D convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_36\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_36 (Embedding)     (None, 31, 300)           8593500   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_33 (Spatia (None, 31, 300)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_257 (Conv1D)          (None, 31, 100)           90100     \n",
      "_________________________________________________________________\n",
      "conv1d_258 (Conv1D)          (None, 31, 100)           30100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_83 (MaxPooling (None, 15, 100)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_259 (Conv1D)          (None, 15, 150)           45150     \n",
      "_________________________________________________________________\n",
      "conv1d_260 (Conv1D)          (None, 15, 150)           67650     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_84 (MaxPooling (None, 7, 150)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_261 (Conv1D)          (None, 7, 150)            67650     \n",
      "_________________________________________________________________\n",
      "conv1d_262 (Conv1D)          (None, 7, 150)            67650     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_85 (MaxPooling (None, 3, 150)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_263 (Conv1D)          (None, 3, 150)            67650     \n",
      "_________________________________________________________________\n",
      "conv1d_264 (Conv1D)          (None, 3, 150)            67650     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_29 (Glo (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 1)                 151       \n",
      "=================================================================\n",
      "Total params: 9,097,251\n",
      "Trainable params: 503,751\n",
      "Non-trainable params: 8,593,500\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 12504 samples, validate on 3126 samples\n",
      "Epoch 1/20\n",
      "12504/12504 [==============================] - 12s 943us/step - loss: 0.4755 - accuracy: 0.7871 - val_loss: 0.4151 - val_accuracy: 0.8218\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.41505, saving model to C:\\Users\\Admin\\Desktop\\CS3244Project\\CNN-kh-n-joyce/model3_postTextweights.01-0.415.hdf5\n",
      "Epoch 2/20\n",
      "12504/12504 [==============================] - 11s 907us/step - loss: 0.4222 - accuracy: 0.8169 - val_loss: 0.4514 - val_accuracy: 0.7991\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.41505\n",
      "Epoch 3/20\n",
      "12504/12504 [==============================] - 11s 903us/step - loss: 0.3955 - accuracy: 0.8264 - val_loss: 0.3902 - val_accuracy: 0.8308\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.41505 to 0.39025, saving model to C:\\Users\\Admin\\Desktop\\CS3244Project\\CNN-kh-n-joyce/model3_postTextweights.03-0.390.hdf5\n",
      "Epoch 4/20\n",
      "12504/12504 [==============================] - 11s 908us/step - loss: 0.3870 - accuracy: 0.8307 - val_loss: 0.3835 - val_accuracy: 0.8317\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.39025 to 0.38348, saving model to C:\\Users\\Admin\\Desktop\\CS3244Project\\CNN-kh-n-joyce/model3_postTextweights.04-0.383.hdf5\n",
      "Epoch 5/20\n",
      "12504/12504 [==============================] - 11s 911us/step - loss: 0.3694 - accuracy: 0.8387 - val_loss: 0.3847 - val_accuracy: 0.8388\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.38348\n",
      "Epoch 6/20\n",
      "12504/12504 [==============================] - 12s 931us/step - loss: 0.3628 - accuracy: 0.8463 - val_loss: 0.3868 - val_accuracy: 0.8413\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.38348\n",
      "Epoch 7/20\n",
      "12504/12504 [==============================] - 12s 926us/step - loss: 0.3474 - accuracy: 0.8526 - val_loss: 0.3847 - val_accuracy: 0.8330\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.38348\n",
      "Epoch 8/20\n",
      "12504/12504 [==============================] - 12s 923us/step - loss: 0.3407 - accuracy: 0.8559 - val_loss: 0.4021 - val_accuracy: 0.8189\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.38348\n",
      "Epoch 9/20\n",
      "12504/12504 [==============================] - 12s 925us/step - loss: 0.3253 - accuracy: 0.8605 - val_loss: 0.3818 - val_accuracy: 0.8401\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.38348 to 0.38182, saving model to C:\\Users\\Admin\\Desktop\\CS3244Project\\CNN-kh-n-joyce/model3_postTextweights.09-0.382.hdf5\n",
      "Epoch 10/20\n",
      "12504/12504 [==============================] - 11s 918us/step - loss: 0.3141 - accuracy: 0.8662 - val_loss: 0.3974 - val_accuracy: 0.8337\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.38182\n",
      "Epoch 11/20\n",
      "12504/12504 [==============================] - 11s 904us/step - loss: 0.3000 - accuracy: 0.8708 - val_loss: 0.4059 - val_accuracy: 0.8212\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.38182\n",
      "Epoch 12/20\n",
      "12504/12504 [==============================] - 11s 898us/step - loss: 0.2864 - accuracy: 0.8776 - val_loss: 0.3956 - val_accuracy: 0.8349\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.38182\n",
      "Epoch 13/20\n",
      "12504/12504 [==============================] - 11s 883us/step - loss: 0.2836 - accuracy: 0.8793 - val_loss: 0.4396 - val_accuracy: 0.8231\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.38182\n",
      "Epoch 14/20\n",
      "12504/12504 [==============================] - 11s 894us/step - loss: 0.2629 - accuracy: 0.8892 - val_loss: 0.4711 - val_accuracy: 0.8141\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.38182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x16f4a252eb8>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hyper-Params\n",
    "epochs = 20\n",
    "batch_size = 32 #for stochastic gradient descent\n",
    "drop_embed = 0.5\n",
    "\n",
    "n_dense = 256\n",
    "dropout = 0.5\n",
    "\n",
    "n_conv_layer1 = 100\n",
    "n_conv_layer2 = 150\n",
    "n_conv_layer3 = 150\n",
    "n_conv_layer4 = 150\n",
    "#n_conv_layer5 = 100\n",
    "k_conv = 3\n",
    "\n",
    "#Define model\n",
    "\n",
    "model3 = Sequential()\n",
    "e = Embedding(vocab_size, vec_dim, weights = [embedding_matrix],\n",
    "              input_length = input_length, trainable = False)\n",
    "model3.add(e)\n",
    "model3.add(SpatialDropout1D(drop_embed))\n",
    "model3.add(Conv1D(filters = n_conv_layer1, kernel_size = k_conv, activation = \"relu\", padding = \"same\"))\n",
    "model3.add(Conv1D(filters = n_conv_layer1, kernel_size = k_conv, activation = \"relu\", padding = \"same\"))\n",
    "model3.add(MaxPooling1D(2))\n",
    "model3.add(Conv1D(filters = n_conv_layer2, kernel_size = k_conv, activation = \"relu\", padding = \"same\"))\n",
    "model3.add(Conv1D(filters = n_conv_layer2, kernel_size = k_conv, activation = \"relu\", padding = \"same\"))\n",
    "model3.add(MaxPooling1D(2))\n",
    "model3.add(Conv1D(filters = n_conv_layer3, kernel_size = k_conv, activation = \"relu\", padding = \"same\"))\n",
    "model3.add(Conv1D(filters = n_conv_layer3, kernel_size = k_conv, activation = \"relu\", padding = \"same\"))\n",
    "model3.add(MaxPooling1D(2))\n",
    "model3.add(Conv1D(filters = n_conv_layer4, kernel_size = k_conv, activation = \"relu\", padding = \"same\"))\n",
    "model3.add(Conv1D(filters = n_conv_layer4, kernel_size = k_conv, activation = \"relu\", padding = \"same\"))\n",
    "#model3.add(MaxPooling1D(2))\n",
    "#model3.add(Conv1D(filters = n_conv_layer5, kernel_size = k_conv, activation = \"relu\", padding = \"same\"))\n",
    "#model3.add(Conv1D(filters = n_conv_layer5, kernel_size = k_conv, activation = \"relu\", padding = \"same\"))\n",
    "model3.add(GlobalMaxPool1D())\n",
    "model3.add(Dropout(dropout))\n",
    "#model3.add(Dense(n_dense, activation = \"relu\"))\n",
    "#model3.add(Dropout(dropout))\n",
    "model3.add(Dense(1, activation = \"sigmoid\"))\n",
    "model3.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "#              loss = \"mean_squared_error\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "### summarize the model\n",
    "print(model3.summary())\n",
    "\n",
    "### Create model checkpoint\n",
    "output_dir = cwd\n",
    "modelcheckpoint = ModelCheckpoint(filepath = output_dir + \"/model3_postTextweights.{epoch:02d}-{val_loss:.3f}.hdf5\",\n",
    "                                 monitor='val_loss', verbose=1, save_best_only=True)\n",
    "callbacks_list = [modelcheckpoint, EarlyStopping(monitor = \"val_loss\", patience = 5)]\n",
    "\n",
    "#callbacks_list_no_chkpt = [EarlyStopping(monitor = \"val_loss\", patience = 2)]\n",
    "\n",
    "### fit the model\n",
    "model3.fit(padded_X_train, y_train, batch_size = batch_size, epochs=epochs, verbose=1, validation_split = 0.2, callbacks = callbacks_list)\n",
    "\n",
    "#fit with no checkpointing\n",
    "#model3.fit(padded_X_train, y_train, batch_size = batch_size, epochs=epochs, verbose=1, validation_split = 0.1, callbacks = callbacks_list_no_chkpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'accuracy']\n",
      "3908/3908 [==============================] - 1s 313us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.505347092783683, 0.7830092310905457]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Evaluate the best model\n",
    "model3_loaded = load_model(\"model3_postTextweights.04-0.373.hdf5\")\n",
    "print(model3_loaded.metrics_names)\n",
    "model3_loaded.evaluate(x=padded_X_test, y=y_test, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
