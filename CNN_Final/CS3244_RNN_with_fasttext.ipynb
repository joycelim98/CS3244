{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CS3244_RNN_with_fasttext.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"code","metadata":{"id":"hpmd2myzIiKw","colab_type":"code","outputId":"25d60e3b-37db-4a4a-fa2c-4142d9f7be0e","executionInfo":{"status":"ok","timestamp":1573298919257,"user_tz":-480,"elapsed":51706,"user":{"displayName":"Sanjukta Saha","photoUrl":"","userId":"11402795547763750428"}},"colab":{"base_uri":"https://localhost:8080/","height":869}},"source":["!pip3 install fasttext\n","!pip3 install pandas numpy tensorflow sklearn scipy six keras matplotlib pydot"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting fasttext\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/61/2e01f1397ec533756c1d893c22d9d5ed3fce3a6e4af1976e0d86bb13ea97/fasttext-0.9.1.tar.gz (57kB)\n","\r\u001b[K     |█████▊                          | 10kB 18.4MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 20kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 30kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 40kB 3.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 51kB 3.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 2.9MB/s \n","\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.4.3)\n","Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (41.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.17.3)\n","Building wheels for collected packages: fasttext\n","  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fasttext: filename=fasttext-0.9.1-cp36-cp36m-linux_x86_64.whl size=2387261 sha256=4923b74567263aacce0a85925f9adc5d0fb14ffe17f2d46b37a124425b12f1bd\n","  Stored in directory: /root/.cache/pip/wheels/9f/f0/04/caa82c912aee89ce76358ff954f3f0729b7577c8ff23a292e3\n","Successfully built fasttext\n","Installing collected packages: fasttext\n","Successfully installed fasttext-0.9.1\n","Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (0.25.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.17.3)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (1.15.0)\n","Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.3.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (1.12.0)\n","Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.2.5)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.1.1)\n","Requirement already satisfied: pydot in /usr/local/lib/python3.6/dist-packages (1.3.0)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.6.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.8)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n","Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.1)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.1)\n","Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.1.0)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.1.7)\n","Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.33.6)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.11.2)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.10.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.21.3)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.8.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.1.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.2)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow) (0.16.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow) (41.4.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow) (3.1.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.14.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_eO_w0zkJWqB","colab_type":"code","outputId":"352cee73-4c17-423b-f73b-7cfddb9d0086","executionInfo":{"status":"ok","timestamp":1573235075774,"user_tz":-480,"elapsed":4817,"user":{"displayName":"Sanjukta Saha","photoUrl":"","userId":"11402795547763750428"}},"colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["!pip3 install jsonlines"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting jsonlines\n","  Downloading https://files.pythonhosted.org/packages/4f/9a/ab96291470e305504aa4b7a2e0ec132e930da89eb3ca7a82fbe03167c131/jsonlines-1.2.0-py2.py3-none-any.whl\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from jsonlines) (1.12.0)\n","Installing collected packages: jsonlines\n","Successfully installed jsonlines-1.2.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7O-jpVEA42cc","outputId":"0c370103-fb84-46f2-f797-3e4b39e605d5","executionInfo":{"status":"error","timestamp":1573298850334,"user_tz":-480,"elapsed":777,"user":{"displayName":"Sanjukta Saha","photoUrl":"","userId":"11402795547763750428"}},"colab":{"base_uri":"https://localhost:8080/","height":478}},"source":["import os\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Flatten, Input, Embedding, LSTM, Dropout\n","from keras.models import Model, Sequential\n","from keras.initializers import Constant\n","from keras.callbacks import ModelCheckpoint, EarlyStopping\n","\n","from sklearn.model_selection import train_test_split\n","\n","from small_dataset import get_small_dataset\n","# from large_dataset import get_large_dataset"],"execution_count":0,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-db2168da98f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msmall_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_small_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;31m# from large_dataset import get_large_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/small_dataset.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mFunction\u001b[0m \u001b[0mcalls\u001b[0m \u001b[0mto\u001b[0m \u001b[0mread\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjsonl\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mto\u001b[0m \u001b[0mpresent\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mjsonlines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'jsonlines'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"g6EiCY8TIiK-","colab_type":"code","outputId":"63e1e607-db3b-474e-c748-1d9cf3dc641d","executionInfo":{"status":"error","timestamp":1573051362669,"user_tz":-480,"elapsed":919,"user":{"displayName":"Sarah Taaher Bonna","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC_AMR8B3WCswU2l1wxVTdEPqMyca6WhfSz1PGTsA=s64","userId":"01288982749819790192"}},"colab":{"base_uri":"https://localhost:8080/","height":324}},"source":["df = get_small_dataset()\n","#df.describe()"],"execution_count":0,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-6c8e7c8ffd17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_small_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#df.describe()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/small_dataset.py\u001b[0m in \u001b[0;36mget_small_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# grandparent_dir = os.path.dirname(parent_dir)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;31m# fileName = os.path.join(grandparent_dir, \"InitialDataset/instances_extracted_SMALL.jsonl\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/small_dataset.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(fileName)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# gets raw data as a np array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mdata_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# convert np array into pandas dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/small_dataset.py\u001b[0m in \u001b[0;36mread_data\u001b[0;34m(fileName)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mjsonlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileName\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0minstance_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jsonlines/jsonlines.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(name, mode, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'mode' must be either 'r', 'w', or 'a'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m     \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m't'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/InitialDataset/instances_extracted_SMALL.jsonl'"]}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"G7_gHTc21EQu","colab":{}},"source":["X = df['title']\n","y = df['label']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4BptjcO7IiLK","colab_type":"code","outputId":"729c8d9a-20d5-4e45-88f7-71e6eade8174","executionInfo":{"status":"error","timestamp":1573051386582,"user_tz":-480,"elapsed":906,"user":{"displayName":"Sarah Taaher Bonna","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC_AMR8B3WCswU2l1wxVTdEPqMyca6WhfSz1PGTsA=s64","userId":"01288982749819790192"}},"colab":{"base_uri":"https://localhost:8080/","height":249}},"source":["\"\"\"\n","Code courtesy of https://keras.io/examples/pretrained_word_embeddings/\n","Inspiration also from https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n","\"\"\"\n","BASE_DIR = ''\n","GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\n","MAX_NUM_WORDS = 20000\n","EMBEDDING_DIM = 100\n","VALIDATION_SPLIT = 0.2\n","\n","# first, build index mapping words in the embeddings set\n","# to their embedding vector\n","\n","print('Indexing word vectors.')\n","\n","embeddings_index = {}\n","with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n","    for line in f:\n","        word, coefs = line.split(maxsplit=1)\n","        coefs = np.fromstring(coefs, 'f', sep=' ')\n","        embeddings_index[word] = coefs\n","\n","print('Found %s word vectors.' % len(embeddings_index))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Indexing word vectors.\n"],"name":"stdout"},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-312b2636a4ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0membeddings_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGLOVE_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'glove.6B.100d.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoefs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxsplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove.6B/glove.6B.100d.txt'"]}]},{"cell_type":"code","metadata":{"id":"TDjdGnP1IiLT","colab_type":"code","colab":{}},"source":["# vectorize the text samples (titles) into a 2D integer tensor\n","tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n","tokenizer.fit_on_texts(X)\n","sequences = tokenizer.texts_to_sequences(X)\n","\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))\n","\n","max_length = max(len(sequences[i]) for i in range(len(sequences)))\n","input_length = max_length + 1\n","\n","padded_X = pad_sequences(sequences, maxlen=input_length, padding=\"post\")\n","\n","#cat_y = to_categorical(np.asarray(y))\n","print('Shape of data tensor:', padded_X.shape)\n","print('Shape of label tensor:', y.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KvYVeh4BIiLd","colab_type":"code","colab":{}},"source":["# split data into training and test set\n","\n","# percentage of test data\n","percentage = 0.2\n","\n","# # split into training and testing data\n","X_train, X_test, y_train, y_test = train_test_split(padded_X, y, test_size = percentage) #random_state = 51\n","\n","\n","\n","print(\"total training examples: %d\\n total test examples: %d\" % (len(X_train), len(X_test)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"op7m_sCVIiLk","colab_type":"code","colab":{}},"source":["# prepare embedding matrix\n","num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n","embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n","for word, i in word_index.items():\n","    if i >= MAX_NUM_WORDS:\n","        continue\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        # words not found in embedding index will be all-zeros.\n","        embedding_matrix[i] = embedding_vector"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gaJGAXyoIiLq","colab_type":"code","colab":{}},"source":["# load pre-trained word embeddings into an Embedding layer\n","# note that we set trainable = False so as to keep the embeddings fixed\n","embedding_layer = Embedding(num_words,\n","                            EMBEDDING_DIM,\n","                            embeddings_initializer=Constant(embedding_matrix),\n","                            input_length=input_length,\n","                            trainable=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BhWRoJp4IiLs","colab_type":"code","colab":{}},"source":["# build model\n","lstm_out = 256\n","dense_out= 64\n","dropout = 0.3\n","\n","sequence_input = Input(shape=(input_length,), dtype='int32')\n","embedded_sequences = embedding_layer(sequence_input)\n","\n","model = Sequential()\n","#model.add(sequence_input)\n","model.add(embedding_layer)\n","model.add(LSTM(lstm_out, dropout=0.1, recurrent_dropout=0.1))\n","# model.add(Dense(dense_out, activation='relu'))\n","# model.add(Dropout(rate=dropout))\n","model.add(Dense(1, activation='relu'))\n","\n","#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","sgd = SGD(lr=0.001, decay=1e-6, momentum=0.5, nesterov=True)\n","model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n","print(model.summary())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sjBTsjqLIiLu","colab_type":"code","colab":{}},"source":["### Create model checkpoint\n","output_dir = os.getcwd()\n","modelcheckpoint = ModelCheckpoint(filepath = output_dir + \"/RNNweights.{epoch:02d}-{val_loss:.2f}.hdf5\",\n","                                 monitor='val_loss', verbose=1, save_best_only=True)\n","callbacks_list = [modelcheckpoint, EarlyStopping(monitor = \"val_loss\", patience = 5)]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"V3K-ZXyRIiLy","colab_type":"code","colab":{}},"source":["#Hyper-Params\n","epochs = 50\n","batch_size = 32\n","\n","\n","# train model\n","history = model.fit(X_train, y_train, batch_size = batch_size, epochs=epochs, verbose=1, validation_split = 0.2, callbacks = callbacks_list)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HWc27ahRIiL0","colab_type":"raw"},"source":["#print validation data\n","# visualizing losses and accuracy\n","train_loss = history.history['loss']\n","val_loss   = history.history['val_loss']\n","train_acc  = history.history['accuracy']\n","val_acc    = history.history['val_accuracy']\n","xc         = range(11)\n","\n","plt.figure()\n","plt.plot(xc, train_loss)\n","plt.plot(xc, val_loss)\n"]},{"cell_type":"code","metadata":{"id":"vGsoeUNHIiL2","colab_type":"code","colab":{}},"source":["# Plot training & validation accuracy values\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('Model accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'], loc='upper left')\n","plt.show()\n","\n","# Plot training & validation loss values\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('Model loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'], loc='upper left')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"d9UAb9qGIiL7","colab_type":"code","colab":{}},"source":["print(model.metrics_names)\n","model.evaluate(X_test, y_test)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TR0058_tIiL-","colab_type":"code","colab":{}},"source":["\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"b7iJ3RxA2QUU","colab":{}},"source":["model = tf.keras.Sequential()\n","model.add(layers.Embedding(input_dim=1000, output_dim=64))\n","\n","# The output of GRU will be a 3D tensor of shape (batch_size, timesteps, 256)\n","model.add(layers.GRU(256, return_sequences=True))\n","\n","# The output of SimpleRNN will be a 2D tensor of shape (batch_size, 128)\n","model.add(layers.SimpleRNN(128))\n","\n","model.add(layers.Dense(10, activation='softmax'))\n","\n","model.summary()"],"execution_count":0,"outputs":[]}]}